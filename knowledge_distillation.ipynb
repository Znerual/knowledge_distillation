{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_linear_schedule_with_warmup\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresearch_projects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistillation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm_seqs_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LmSeqsDataset\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m LooseVersion(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m LooseVersion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import time \n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.examples.research_projects.distillation.lm_seqs_dataset import LmSeqsDataset\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gopher_rules_pass(sample) -> bool:\n",
    "    \"\"\" function returns True if the sample complies with Gopher rules \"\"\"\n",
    "    signals = json.loads(sample[\"quality_signals\"])\n",
    "\n",
    "    # rule 1: number of words between 50 and 10'000\n",
    "    word_count = signals[\"rps_doc_word_count\"][0][2]\n",
    "    if word_count < 50 or word_count > 10_000:\n",
    "        return False\n",
    "\n",
    "    # rule 2: mean word length between 3 and 10\n",
    "    mean_word_length = signals[\"rps_doc_mean_word_length\"][0][2]\n",
    "    if mean_word_length < 3 or mean_word_length > 10:\n",
    "        return False\n",
    "\n",
    "    # rule 2: symbol to word ratio below 0.1\n",
    "    symbol_word_ratio = signals[\"rps_doc_symbol_to_word_ratio\"][0][2]\n",
    "    if symbol_word_ratio > 0.1:\n",
    "        return False\n",
    "\n",
    "    # rule 3: 90% of lines need to start without a bullet point\n",
    "    n_lines = signals[\"ccnet_nlines\"][0][2]\n",
    "    n_lines_bulletpoint_start = sum(map(lambda ln: ln[2], signals[\"rps_lines_start_with_bulletpoint\"]))\n",
    "    if n_lines_bulletpoint_start / n_lines > 0.9:\n",
    "        return False\n",
    "\n",
    "    # rule 4: the ratio between characters in the most frequent 2-gram and the total number \n",
    "    # of characters must be below 0.2\n",
    "    top_2_gram_frac = signals[\"rps_doc_frac_chars_top_2gram\"][0][2]\n",
    "    if top_2_gram_frac > 0.2:\n",
    "        return False\n",
    "\n",
    "    # rule 5: ...\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_iterator \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogethercomputer/RedPajama-Data-V2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   partition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_middle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                   \u001b[38;5;66;03m#snapshots=[\"2023-06\", \"2022-49\"],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                   languages\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m                   streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,) \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m ds_iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mgopher_rules_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     documents \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mgopher_rules_pass\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgopher_rules_pass\u001b[39m(sample) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" function returns True if the sample complies with Gopher rules \"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     signals \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mloads(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquality_signals\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# rule 1: number of words between 50 and 10'000\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m signals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrps_doc_word_count\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds_iterator = load_dataset(\"togethercomputer/RedPajama-Data-V2\",\n",
    "                  name=\"default\",\n",
    "                  partition=\"head_middle\",\n",
    "                  #snapshots=[\"2023-06\", \"2022-49\"],\n",
    "                  languages=[\"en\", \"de\", \"fr\", \"es\", \"it\"],\n",
    "                  streaming=True,) \n",
    "\n",
    "for sample in ds_iterator[\"train\"]:\n",
    "\n",
    "    if not gopher_rules_pass(sample):\n",
    "        continue\n",
    "\n",
    "    documents = json.loads(sample[\"documents\"])\n",
    "\n",
    "    print(documents)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT AWB\n",
      "KIDS ARE KIDS\n",
      "JOIN THE CAST\n",
      "<< Back to AWB News\n",
      "Christine Rouse is honored on the “Today Show”\n",
      "The executive director of Acting Without Boundaries (AWB), Christine Rouse, was featured on the NBC Today Show with “Kathie Lee and Hoda” on March 1, 2012. The monthly segment, called “Everyone Has A Story,” features one ordinary person that has had a life-changing experience in their own life. Christine submitted an essay describing her life’s mission of increasing awareness of and support for people with disabilities. She described the process of creating the two non-profits she manages – “Kids are Kids,” which provides disability awareness workshops and AWB which provides theater arts opportunities for children, youth and young adults with physical disabilities . Christine talked about the importance of both in increasing inclusion for people, especially young people, with physical disabilities.\n",
      "The March “Everyone Has A Story” segment featured Christine, her mother, and her brother. Christine’s mother read a letter she wrote about Christine’s life and the pride she takes in her many accomplishments. John Tartaglia, a Broadway performer sang a song written for Christine – “Different is Beautiful”- by Kathie Lee Gifford and David Freidman. The song has a powerful message and will be performed by AWB actors in the near future.\n",
      "To cap off this exciting experience, the Today Show honored Christine’s work with $1000 donations to each of her organizations, Kids are Kids and AWB.\n",
      "750 E. Haverford Road, Bryn Mawr, PA 19010\n",
      "Email: mmurphy@awb2004.org\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"raw_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller:\n",
    "\tdef __init__(self, params, student_model, teacher_model, dataloader):\n",
    "\t\tself.alpha_ce = params.get(\"alpha_ce\", 0.5)\n",
    "\t\tself.alpha_mlm = params.get(\"alpha_mlm\", 0.5)\n",
    "\t\tself.alpha_clm = params.get(\"alpha_clm\", 0.5)\n",
    "\t\tself.alpha_mse = params.get(\"alpha_mse\", 0.0)\n",
    "\t\tself.alpha_cos = params.alpha_cos\n",
    "\n",
    "\t\tself.temperature = params.get(\"temperature\", 2.0)\n",
    "\n",
    "\t\tself.mlm_mask_prob = params.get(\"mask_prob\", 0.15)\n",
    "\t\tself.word_rand = params.get(\"word_rand\", 0.1)\n",
    "\t\tself.word_keep = params.get(\"word_keep\", 0.1)\n",
    "\t\tself.word_mask = params.get(\"word_mask\", 0.8)\n",
    "\t\tassert self.word_rand + self.word_keep + self.word_mask == 1.0\n",
    "\n",
    "\t\tself.n_epoch = params.n_epoch\n",
    "\t\tself.batch_size = params.batch_size\n",
    "\t\tself.gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 50)\n",
    "\n",
    "\t\tself.warmup_prop = params.get(\"warmup_prop\", 0.05)\n",
    "\t\tself.weight_decay = params.get(\"weight_decay\", 0.0)\n",
    "\t\tself.learning_rate = params.get(\"learning_rate\", 5e-4)\n",
    "\t\tself.adam_epsilon = params.get(\"adam_epsilon\", 1e-6)\n",
    "\t\tself.max_grad_norm = params.get(\"max_grad_norm\", 5.0)\n",
    "\t\tself.initializer_range = params.get(\"initializer_range\", 0.02)\n",
    "\n",
    "\t\tself.student = student_model\n",
    "\t\tself.teacher = teacher_model\n",
    "\n",
    "\t\tself.dataloader = dataloader\n",
    "\n",
    "\t\tself.epoch = 0\n",
    "\t\tself.n_iter = 0\n",
    "\t\tself.n_total_iter = 0\n",
    "\t\tself.n_sequences_epoch = 0\n",
    "\t\tself.total_loss_epoch = 0\n",
    "\t\tself.last_loss = 0\n",
    "\t\tself.last_loss_ce = 0\n",
    "\t\tself.last_loss_clm = 0\n",
    "\t\tif self.alpha_mse > 0.0:\n",
    "\t\t\tself.last_loss_mse = 0\n",
    "\t\tif self.alpha_cos > 0.0:\n",
    "\t\t\tself.last_loss_cos = 0\n",
    "\t\tself.last_log = 0\n",
    "\n",
    "\t\tself.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\t\tself.lm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\t\tif self.alpha_mse > 0.0:\n",
    "\t\t\tself.mse_loss_fct = nn.MSELoss(reduction=\"sum\")\n",
    "\t\tif self.alpha_cos > 0.0:\n",
    "\t\t\tself.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "\t\tlogger.info(\"--- Initializing model optimizer\")\n",
    "\t\tassert params.gradient_accumulation_steps >= 1\n",
    "\t\tself.num_steps_epoch = len(self.dataloader)\n",
    "\t\tnum_train_optimization_steps = (\n",
    "\t\t\tint(self.num_steps_epoch / params.gradient_accumulation_steps * params.n_epoch) + 1\n",
    "\t\t)\n",
    "\n",
    "\t\tno_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\t\toptimizer_grouped_parameters = [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"params\": [\n",
    "\t\t\t\t\tp for n, p in student_model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"weight_decay\": params.weight_decay,\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"params\": [\n",
    "\t\t\t\t\tp for n, p in student_model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"weight_decay\": 0.0,\n",
    "\t\t\t},\n",
    "\t\t]\n",
    "\t\tlogger.info(\n",
    "\t\t\t\"------ Number of trainable parameters (student): %i\"\n",
    "\t\t\t% sum([p.numel() for p in self.student.parameters() if p.requires_grad])\n",
    "\t\t)\n",
    "\t\tlogger.info(\"------ Number of parameters (student): %i\" % sum([p.numel() for p in self.student.parameters()]))\n",
    "\t\tself.optimizer = AdamW(\n",
    "\t\t\toptimizer_grouped_parameters, lr=params.learning_rate, eps=params.adam_epsilon, betas=(0.9, 0.98)\n",
    "\t\t)\n",
    "\n",
    "\t\twarmup_steps = math.ceil(num_train_optimization_steps * params.warmup_prop)\n",
    "\t\tself.scheduler = get_linear_schedule_with_warmup(\n",
    "\t\t\tself.optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps\n",
    "\t\t)\n",
    "\n",
    "\t\tlogger.info(\"--- Initializing Tensorboard\")\n",
    "\t\tself.tensorboard = SummaryWriter(log_dir=os.path.join(self.dump_path, \"log\", \"train\"))\n",
    "\t\tself.tensorboard.add_text(tag=\"config/training\", text_string=str(self.params), global_step=0)\n",
    "\t\tself.tensorboard.add_text(tag=\"config/student\", text_string=str(self.student_config), global_step=0)\n",
    "\n",
    "\n",
    "\tdef round_batch(self, x: torch.tensor, lengths: torch.tensor):\n",
    "\t\t\"\"\"\n",
    "\t\tFor float16 only.\n",
    "\t\tSub-sample sentences in a batch, and add padding, so that each dimension is a multiple of 8.\n",
    "\n",
    "\t\tInput:\n",
    "\t\t------\n",
    "\t\t\tx: `torch.tensor(bs, seq_length)` - The token ids.\n",
    "\t\t\tlengths: `torch.tensor(bs, seq_length)` - The lengths of each of the sequence in the batch.\n",
    "\n",
    "\t\tOutput:\n",
    "\t\t-------\n",
    "\t\t\tx:  `torch.tensor(new_bs, new_seq_length)` - The updated token ids.\n",
    "\t\t\tlengths: `torch.tensor(new_bs, new_seq_length)` - The updated lengths.\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.fp16 or len(lengths) < 8:\n",
    "\t\t\treturn x, lengths\n",
    "\n",
    "\t\t# number of sentences == 0 [8]\n",
    "\t\tbs1 = len(lengths)\n",
    "\t\tbs2 = 8 * (bs1 // 8)\n",
    "\t\tassert bs2 > 0 and bs2 % 8 == 0\n",
    "\t\tif bs1 != bs2:\n",
    "\t\t\tidx = torch.randperm(bs1)[:bs2]\n",
    "\t\t\tlengths = lengths[idx]\n",
    "\t\t\tslen = lengths.max().item()\n",
    "\t\t\tx = x[idx, :slen]\n",
    "\t\telse:\n",
    "\t\t\tidx = None\n",
    "\n",
    "\t\t# sequence length == 0 [8]\n",
    "\t\tml1 = x.size(1)\n",
    "\t\tif ml1 % 8 != 0:\n",
    "\t\t\tpad = 8 - (ml1 % 8)\n",
    "\t\t\tml2 = ml1 + pad\n",
    "\t\t\tif self.mlm:\n",
    "\t\t\t\tpad_id = self.params.special_tok_ids[\"pad_token\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tpad_id = self.params.special_tok_ids[\"unk_token\"]\n",
    "\t\t\tpadding_tensor = torch.zeros(bs2, pad, dtype=torch.long, device=x.device).fill_(pad_id)\n",
    "\t\t\tx = torch.cat([x, padding_tensor], 1)\n",
    "\t\t\tassert x.size() == (bs2, ml2)\n",
    "\n",
    "\t\tassert x.size(0) % 8 == 0\n",
    "\t\tassert x.size(1) % 8 == 0\n",
    "\t\treturn x, lengths\n",
    "\t\n",
    "\tdef prepare_batch(self, batch):\n",
    "\t\t\"\"\"\n",
    "\t\tPrepare the batch: from the token_ids and the lengths, compute the attention mask and the labels for CLM.\n",
    "\n",
    "\t\tInput:\n",
    "\t\t------\n",
    "\t\t\tbatch: `Tuple`\n",
    "\t\t\t\ttoken_ids: `torch.tensor(bs, seq_length)` - The token ids for each of the sequence. It is padded.\n",
    "\t\t\t\tlengths: `torch.tensor(bs)` - The lengths of each of the sequences in the batch.\n",
    "\n",
    "\t\tOutput:\n",
    "\t\t-------\n",
    "\t\t\ttoken_ids: `torch.tensor(bs, seq_length)` - The token ids after the modifications for MLM.\n",
    "\t\t\tattn_mask: `torch.tensor(bs, seq_length)` - The attention mask for the self-attention.\n",
    "\t\t\tclm_labels: `torch.tensor(bs, seq_length)` - The causal language modeling labels. There is a -100 where there is nothing to predict.\n",
    "\t\t\"\"\"\n",
    "\t\ttoken_ids, lengths = batch\n",
    "\t\ttoken_ids, lengths = self.round_batch(x=token_ids, lengths=lengths)\n",
    "\t\tassert token_ids.size(0) == lengths.size(0)\n",
    "\n",
    "\t\tattn_mask = torch.arange(token_ids.size(1), dtype=torch.long, device=lengths.device) < lengths[:, None]\n",
    "\t\tclm_labels = token_ids.new(token_ids.size()).copy_(token_ids)\n",
    "\t\tclm_labels[~attn_mask] = -100  # previously `clm_labels[1-attn_mask] = -1`, cf pytorch 1.2.0 compatibility\n",
    "\n",
    "\t\t# sanity checks\n",
    "\t\tassert 0 <= token_ids.min() <= token_ids.max() < self.vocab_size\n",
    "\n",
    "\t\treturn token_ids, attn_mask, clm_labels\n",
    "\t\n",
    "\tdef train(self):\n",
    "\t\tself.student.train()\n",
    "\t\tself.teacher.eval()\n",
    "\n",
    "\t\tfor _ in range(self.n_epoch):\n",
    "\t\t\titer_bar = tqdm(self.dataloader, desc=\"-Iter\", disable=self.params.local_rank not in [-1, 0])\n",
    "\t\t\tfor batch in iter_bar:\n",
    "\t\t\t\tinput_ids, attn_mask, lm_labels = self.prepare_batch(batch) \n",
    "\t\t\t\tself.step(input_ids, attn_mask, lm_labels)\n",
    "\n",
    "\t\t\t\titer_bar.update()\n",
    "\t\t\t\titer_bar.set_postfix(\n",
    "\t\t\t\t\t{\"Last_loss\": f\"{self.last_loss:.2f}\", \"Avg_cum_loss\": f\"{self.total_loss_epoch/self.n_iter:.2f}\"}\n",
    "\t\t\t\t)\n",
    "\t\t\titer_bar.close()\n",
    "\t\t\tlogger.info(f\"--- Ending epoch {self.epoch}/{self.params.n_epoch-1}\")\n",
    "\t\t\tself.end_epoch()\n",
    "\n",
    "\t\tlogger.info(\"Save very last checkpoint as `pytorch_model.bin`.\")\n",
    "\t\tself.save_checkpoint(checkpoint_name=\"pytorch_model.bin\")\n",
    "\t\tlogger.info(\"Training is finished\")\n",
    "\n",
    "\n",
    "\tdef step(self, input_ids  : torch.tensor,  attention_mask : torch.tensor, lm_labels : torch.tensor):\n",
    "\t\t\"\"\"\n",
    "\t\tPerforms a single training step\n",
    "\t\t\"\"\"\n",
    "\t\ts_logits = self.student(input_ids=input_ids, attention_mask=None)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tt_logits = self.teacher(input_ids=input_ids, attention_mask=None)\n",
    "\n",
    "\t\tassert s_logits.size() == t_logits.size()\n",
    "\n",
    "\t\tmask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)\n",
    "\t\ts_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "\t\ts_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "\t\tt_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n",
    "\t\tt_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n",
    "\t\tassert t_logits_slct.size() == s_logits_slct.size()\n",
    "\n",
    "\t\tloss_ce = (\n",
    "\t\t\tself.ce_loss_fct(\n",
    "\t\t\t\tnn.functional.log_softmax(s_logits_slct / self.temperature, dim=-1),\n",
    "\t\t\t\tnn.functional.softmax(t_logits_slct / self.temperature, dim=-1),\n",
    "\t\t\t)\n",
    "\t\t\t* (self.temperature) ** 2\n",
    "\t\t)\n",
    "\t\tloss = self.alpha_ce * loss_ce\n",
    "\n",
    "\t\tif self.alpha_mlm > 0.0:\n",
    "\t\t\tloss_mlm = self.lm_loss_fct(s_logits.view(-1, s_logits.size(-1)), lm_labels.view(-1))\n",
    "\t\t\tloss += self.alpha_mlm * loss_mlm\n",
    "\n",
    "\t\tif self.alpha_clm > 0.0:\n",
    "\t\t\tshift_logits = s_logits[..., :-1, :].contiguous()\n",
    "\t\t\tshift_labels = lm_labels[..., 1:].contiguous()\n",
    "\t\t\tloss_clm = self.lm_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\t\t\tloss += self.alpha_clm * loss_clm\n",
    "\n",
    "\t\tif self.alpha_mse > 0.0:\n",
    "\t\t\tloss_mse = self.mse_loss_fct(s_logits_slct, t_logits_slct) / s_logits_slct.size(\n",
    "\t\t\t\t0\n",
    "\t\t\t)  # Reproducing batchmean reduction\n",
    "\t\t\tloss += self.alpha_mse * loss_mse\n",
    "\n",
    "\t\tself.total_loss_epoch += loss.item()\n",
    "\t\tself.last_loss = loss.item()\n",
    "\t\tself.last_loss_ce = loss_ce.item()\n",
    "\t\tif self.alpha_mlm > 0.0:\n",
    "\t\t\tself.last_loss_mlm = loss_mlm.item()\n",
    "\t\tif self.alpha_clm > 0.0:\n",
    "\t\t\tself.last_loss_clm = loss_clm.item()\n",
    "\t\tif self.alpha_mse > 0.0:\n",
    "\t\t\tself.last_loss_mse = loss_mse.item()\n",
    "\n",
    "\t\tself.optimize(loss)\n",
    "\n",
    "\t\tself.n_sequences_epoch += input_ids.size(0)\n",
    "\n",
    "\tdef optimize(self, loss):\n",
    "\t\t\"\"\"\n",
    "\t\tNormalization on the loss (gradient accumulation or distributed training), followed by\n",
    "\t\tbackward pass on the loss, possibly followed by a parameter update (depending on the gradient accumulation).\n",
    "\t\tAlso update the metrics for tensorboard.\n",
    "\t\t\"\"\"\n",
    "\t\t# Check for NaN\n",
    "\t\tif (loss != loss).data.any():\n",
    "\t\t\tlogger.error(\"NaN detected\")\n",
    "\t\t\texit()\n",
    "\n",
    "\t\tif self.params.gradient_accumulation_steps > 1:\n",
    "\t\t\tloss = loss / self.params.gradient_accumulation_steps\n",
    "\n",
    "\t\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\tself.iter()\n",
    "\t\tif self.n_iter % self.params.gradient_accumulation_steps == 0:\n",
    "\t\t\t\n",
    "\t\t\tnn.utils.clip_grad_norm_(self.student.parameters(), self.params.max_grad_norm)\n",
    "\t\t\tself.optimizer.step()\n",
    "\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\tself.scheduler.step()\n",
    "\n",
    "\tdef iter(self):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdate global counts, write to tensorboard and save checkpoint.\n",
    "\t\t\"\"\"\n",
    "\t\tself.n_iter += 1\n",
    "\t\tself.n_total_iter += 1\n",
    "\n",
    "\t\tif self.n_total_iter % self.params.log_interval == 0:\n",
    "\t\t\tself.log_tensorboard()\n",
    "\t\t\tself.last_log = time.time()\n",
    "\t\tif self.n_total_iter % self.params.checkpoint_interval == 0:\n",
    "\t\t\tself.save_checkpoint()\n",
    "\n",
    "\tdef log_tensorboard(self):\n",
    "\t\t\"\"\"\n",
    "\t\tLog into tensorboard. Only by the master process.\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.is_master:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tfor param_name, param in self.student.named_parameters():\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"parameter_mean/\" + param_name, scalar_value=param.data.mean(), global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"parameter_std/\" + param_name, scalar_value=param.data.std(), global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\t\tif param.grad is None:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"grad_mean/\" + param_name, scalar_value=param.grad.data.mean(), global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"grad_std/\" + param_name, scalar_value=param.grad.data.std(), global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.tensorboard.add_scalar(\n",
    "\t\t\ttag=\"losses/cum_avg_loss_epoch\",\n",
    "\t\t\tscalar_value=self.total_loss_epoch / self.n_iter,\n",
    "\t\t\tglobal_step=self.n_total_iter,\n",
    "\t\t)\n",
    "\t\tself.tensorboard.add_scalar(tag=\"losses/loss\", scalar_value=self.last_loss, global_step=self.n_total_iter)\n",
    "\t\tself.tensorboard.add_scalar(\n",
    "\t\t\ttag=\"losses/loss_ce\", scalar_value=self.last_loss_ce, global_step=self.n_total_iter\n",
    "\t\t)\n",
    "\t\tif self.alpha_mlm > 0.0:\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"losses/loss_mlm\", scalar_value=self.last_loss_mlm, global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\tif self.alpha_clm > 0.0:\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"losses/loss_clm\", scalar_value=self.last_loss_clm, global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\tif self.alpha_mse > 0.0:\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"losses/loss_mse\", scalar_value=self.last_loss_mse, global_step=self.n_total_iter\n",
    "\t\t\t)\n",
    "\t\t\n",
    "\t\tself.tensorboard.add_scalar(\n",
    "\t\t\ttag=\"learning_rate/lr\", scalar_value=self.scheduler.get_lr()[0], global_step=self.n_total_iter\n",
    "\t\t)\n",
    "\n",
    "\t\tself.tensorboard.add_scalar(\n",
    "\t\t\ttag=\"global/memory_usage\",\n",
    "\t\t\tscalar_value=psutil.virtual_memory()._asdict()[\"used\"] / 1_000_000,\n",
    "\t\t\tglobal_step=self.n_total_iter,\n",
    "\t\t)\n",
    "\t\tself.tensorboard.add_scalar(\n",
    "\t\t\ttag=\"global/speed\", scalar_value=time.time() - self.last_log, global_step=self.n_total_iter\n",
    "\t\t)\n",
    "\n",
    "\tdef end_epoch(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFinally arrived at the end of epoch (full pass on dataset).\n",
    "\t\tDo some tensorboard logging and checkpoint saving.\n",
    "\t\t\"\"\"\n",
    "\t\tlogger.info(f\"{self.n_sequences_epoch} sequences have been trained during this epoch.\")\n",
    "\n",
    "\t\tif self.is_master:\n",
    "\t\t\tself.save_checkpoint(checkpoint_name=f\"model_epoch_{self.epoch}.pth\")\n",
    "\t\t\tself.tensorboard.add_scalar(\n",
    "\t\t\t\ttag=\"epoch/loss\", scalar_value=self.total_loss_epoch / self.n_iter, global_step=self.epoch\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.epoch += 1\n",
    "\t\tself.n_sequences_epoch = 0\n",
    "\t\tself.n_iter = 0\n",
    "\t\tself.total_loss_epoch = 0\n",
    "\n",
    "\tdef save_checkpoint(self, checkpoint_name: str = \"checkpoint.pth\"):\n",
    "\t\t\"\"\"\n",
    "\t\tSave the current state. Only by the master process.\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.is_master:\n",
    "\t\t\treturn\n",
    "\t\tmdl_to_save = self.student.module if hasattr(self.student, \"module\") else self.student\n",
    "\t\tmdl_to_save.config.save_pretrained(self.dump_path)\n",
    "\t\tstate_dict = mdl_to_save.state_dict()\n",
    "\t\ttorch.save(state_dict, os.path.join(self.dump_path, checkpoint_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
