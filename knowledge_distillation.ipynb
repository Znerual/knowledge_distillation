{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gopher_rules_pass(sample) -> bool:\n",
    "    \"\"\" function returns True if the sample complies with Gopher rules \"\"\"\n",
    "    signals = json.loads(sample[\"quality_signals\"])\n",
    "\n",
    "    # rule 1: number of words between 50 and 10'000\n",
    "    word_count = signals[\"rps_doc_word_count\"][0][2]\n",
    "    if word_count < 50 or word_count > 10_000:\n",
    "        return False\n",
    "\n",
    "    # rule 2: mean word length between 3 and 10\n",
    "    mean_word_length = signals[\"rps_doc_mean_word_length\"][0][2]\n",
    "    if mean_word_length < 3 or mean_word_length > 10:\n",
    "        return False\n",
    "\n",
    "    # rule 2: symbol to word ratio below 0.1\n",
    "    symbol_word_ratio = signals[\"rps_doc_symbol_to_word_ratio\"][0][2]\n",
    "    if symbol_word_ratio > 0.1:\n",
    "        return False\n",
    "\n",
    "    # rule 3: 90% of lines need to start without a bullet point\n",
    "    n_lines = signals[\"ccnet_nlines\"][0][2]\n",
    "    n_lines_bulletpoint_start = sum(map(lambda ln: ln[2], signals[\"rps_lines_start_with_bulletpoint\"]))\n",
    "    if n_lines_bulletpoint_start / n_lines > 0.9:\n",
    "        return False\n",
    "\n",
    "    # rule 4: the ratio between characters in the most frequent 2-gram and the total number \n",
    "    # of characters must be below 0.2\n",
    "    top_2_gram_frac = signals[\"rps_doc_frac_chars_top_2gram\"][0][2]\n",
    "    if top_2_gram_frac > 0.2:\n",
    "        return False\n",
    "\n",
    "    # rule 5: ...\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_iterator \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogethercomputer/RedPajama-Data-V2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   partition\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_middle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                   \u001b[38;5;66;03m#snapshots=[\"2023-06\", \"2022-49\"],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                   languages\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m                   streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,) \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m ds_iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mgopher_rules_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     documents \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mgopher_rules_pass\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgopher_rules_pass\u001b[39m(sample) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" function returns True if the sample complies with Gopher rules \"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     signals \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mloads(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquality_signals\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# rule 1: number of words between 50 and 10'000\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     word_count \u001b[38;5;241m=\u001b[39m signals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrps_doc_word_count\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds_iterator = load_dataset(\"togethercomputer/RedPajama-Data-V2\",\n",
    "                  name=\"default\",\n",
    "                  partition=\"head_middle\",\n",
    "                  #snapshots=[\"2023-06\", \"2022-49\"],\n",
    "                  languages=[\"en\", \"de\", \"fr\", \"es\", \"it\"],\n",
    "                  streaming=True,) \n",
    "\n",
    "for sample in ds_iterator[\"train\"]:\n",
    "\n",
    "    if not gopher_rules_pass(sample):\n",
    "        continue\n",
    "\n",
    "    documents = json.loads(sample[\"documents\"])\n",
    "\n",
    "    print(documents)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT AWB\n",
      "KIDS ARE KIDS\n",
      "JOIN THE CAST\n",
      "<< Back to AWB News\n",
      "Christine Rouse is honored on the “Today Show”\n",
      "The executive director of Acting Without Boundaries (AWB), Christine Rouse, was featured on the NBC Today Show with “Kathie Lee and Hoda” on March 1, 2012. The monthly segment, called “Everyone Has A Story,” features one ordinary person that has had a life-changing experience in their own life. Christine submitted an essay describing her life’s mission of increasing awareness of and support for people with disabilities. She described the process of creating the two non-profits she manages – “Kids are Kids,” which provides disability awareness workshops and AWB which provides theater arts opportunities for children, youth and young adults with physical disabilities . Christine talked about the importance of both in increasing inclusion for people, especially young people, with physical disabilities.\n",
      "The March “Everyone Has A Story” segment featured Christine, her mother, and her brother. Christine’s mother read a letter she wrote about Christine’s life and the pride she takes in her many accomplishments. John Tartaglia, a Broadway performer sang a song written for Christine – “Different is Beautiful”- by Kathie Lee Gifford and David Freidman. The song has a powerful message and will be performed by AWB actors in the near future.\n",
      "To cap off this exciting experience, the Today Show honored Christine’s work with $1000 donations to each of her organizations, Kids are Kids and AWB.\n",
      "750 E. Haverford Road, Bryn Mawr, PA 19010\n",
      "Email: mmurphy@awb2004.org\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"raw_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller:\n",
    "    def __init__(self, params student_model, teacher_model, dataset):\n",
    "        self.alpha_ce = params.get(\"alpha_ce\", 0.5)\n",
    "        self.alpha_mlm = params.get(\"alpha_mlm\", 0.0)\n",
    "        self.alpha_clm = params.get(\"alpha_clm\", 0.5)\n",
    "        self.alpha_mse = params.get(\"alpha_mse\", 0.0)\n",
    "        self.alpha_cos = params.alpha_cos\n",
    "\n",
    "        self.temperature = params.get(\"temperature\", 2.0)\n",
    "\n",
    "        self.mlm_mask_prob = params.get(\"mask_prob\", 0.15)\n",
    "        self.word_rand = params.get(\"word_rand\", 0.1)\n",
    "        self.word_keep = params.get(\"word_keep\", 0.1)\n",
    "        self.word_mask = params.get(\"word_mask\", 0.8)\n",
    "        assert self.word_rand + self.word_keep + self.word_mask == 1.0\n",
    "\n",
    "        self.n_epoch = params.n_epoch\n",
    "        self.batch_size = params.batch_size\n",
    "        self.gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 50)\n",
    "\n",
    "        self.warmup_prop = params.get(\"warmup_prop\", 0.05)\n",
    "        self.weight_decay = params.get(\"weight_decay\", 0.0)\n",
    "        self.learning_rate = params.get(\"learning_rate\", 5e-4)\n",
    "        self.adam_epsilon = params.get(\"adam_epsilon\", 1e-6)\n",
    "        self.max_grad_norm = params.get(\"max_grad_norm\", 5.0)\n",
    "        self.initializer_range = params.get(\"initializer_range\", 0.02)\n",
    "\n",
    "\n",
    "        self.student_model = student_model\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
