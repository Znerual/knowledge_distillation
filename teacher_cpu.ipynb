{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU + GPU Offloading\n",
    "Install lama.cpp-python with cuBLAS:\n",
    "```bash\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
    "```\n",
    "or better, use ctransformers:\n",
    "```\n",
    "pip install ctransformers[cuda]\n",
    "pip install ctransformers[gptq]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 13231.24it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create LLM 'phi' from '/home/ruzickal/.cache/huggingface/hub/models--TheBloke--phi-2-GGUF/blobs/ce6bcc8fa75b467e1eb693bcc921fe366ceac14aa92ec3dc84da536c2289510a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Yi-6B-200K-GGUF\", model_file=\"yi-6b-200k.Q5_K_M.gguf\", model_type=\"yi\", gpu_layers=12) # hf=True\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\", model_file=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=12) \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\", model_file=\"mistral-7b-v0.1.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=8) \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheBloke/phi-2-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphi-2.Q5_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(llm)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(llm(\"AI is going to \"))\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/ctransformers/hub.py:175\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    169\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    170\u001b[0m         model_file,\n\u001b[1;32m    171\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    172\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hf:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/ctransformers/llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    248\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    249\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    250\u001b[0m     config\u001b[38;5;241m.\u001b[39mto_struct(),\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to create LLM \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m architecture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctransformers_llm_architecture()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create LLM 'phi' from '/home/ruzickal/.cache/huggingface/hub/models--TheBloke--phi-2-GGUF/blobs/ce6bcc8fa75b467e1eb693bcc921fe366ceac14aa92ec3dc84da536c2289510a'."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Yi-6B-200K-GGUF\", model_file=\"yi-6b-200k.Q5_K_M.gguf\", model_type=\"yi\", gpu_layers=12) # hf=True\n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\", model_file=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=12) \n",
    "#llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-GGUF\", model_file=\"mistral-7b-v0.1.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=8) \n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/phi-2-GGUF\", model_file=\"phi-2.Q5_K_M.gguf\", model_type=\"phi\", gpu_layers=100) \n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "#print(llm(\"AI is going to \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: Quadro M3000M, compute capability 5.2\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from /home/ruzickal/.cache/huggingface/hub/models--TheBloke--phi-2-GGUF/snapshots/5a454d977c6438bb9fb2df233c8ca70f21c87420/phi-2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_K     [  2560, 51200,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:             blk.0.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:            blk.0.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:           blk.0.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:                blk.0.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:            blk.0.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:             blk.1.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:           blk.1.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:           blk.1.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:                blk.1.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.10.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:          blk.10.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:             blk.10.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:           blk.10.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:          blk.10.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:        blk.10.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:               blk.10.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:             blk.10.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:             blk.10.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.10.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:            blk.11.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:          blk.11.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.11.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:          blk.11.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:        blk.11.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:               blk.11.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:             blk.11.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:             blk.11.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:           blk.11.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.12.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:          blk.12.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:             blk.12.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.12.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:          blk.12.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:        blk.12.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:               blk.12.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:             blk.12.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.12.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.12.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.13.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:          blk.13.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:           blk.13.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.13.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:        blk.13.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:               blk.13.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.13.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:             blk.13.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:           blk.13.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.14.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:          blk.14.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.14.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:          blk.14.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:        blk.14.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:               blk.14.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:             blk.14.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.14.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.14.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.15.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:          blk.15.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:             blk.15.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.15.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:          blk.15.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:        blk.15.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:               blk.15.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.15.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:             blk.15.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.15.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.16.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.16.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:             blk.16.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.16.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:          blk.16.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:        blk.16.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:               blk.16.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:             blk.16.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.16.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.16.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:            blk.17.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:          blk.17.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.17.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:           blk.17.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:          blk.17.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.17.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:               blk.17.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.17.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.17.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:           blk.17.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:            blk.18.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:          blk.18.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.18.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.18.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:          blk.18.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.18.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:               blk.18.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.18.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.18.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.18.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.19.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:          blk.19.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.19.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.19.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.19.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:        blk.19.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:               blk.19.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.19.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.19.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.19.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.2.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.2.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:              blk.2.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:            blk.2.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.2.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:         blk.2.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:                blk.2.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:              blk.2.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:              blk.2.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:            blk.2.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:            blk.20.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:          blk.20.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.20.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.20.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:          blk.20.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:        blk.20.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:               blk.20.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.20.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.20.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.20.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:            blk.21.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.21.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.21.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.21.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.21.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:        blk.21.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:               blk.21.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.21.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.21.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.21.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:            blk.22.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.22.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.22.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.22.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:          blk.22.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:        blk.22.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:               blk.22.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.22.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.22.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.22.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:            blk.23.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:          blk.23.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.23.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.23.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:          blk.23.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.23.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:               blk.23.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.23.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.23.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.23.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:            blk.24.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.24.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.24.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.24.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:          blk.24.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:        blk.24.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:               blk.24.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.24.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.24.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.24.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:            blk.25.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:          blk.25.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.25.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:           blk.25.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:          blk.25.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.25.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:               blk.25.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.25.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.25.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.25.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.26.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:          blk.26.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.26.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.26.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:          blk.26.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:        blk.26.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:               blk.26.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.26.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.26.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.26.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.27.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:          blk.27.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.27.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.27.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.27.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:        blk.27.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:               blk.27.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.27.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.27.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.27.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:            blk.28.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:          blk.28.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.28.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.28.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.28.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:        blk.28.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:               blk.28.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.28.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.28.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.28.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:            blk.29.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:          blk.29.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.29.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.29.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:          blk.29.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:        blk.29.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:               blk.29.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.29.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.29.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.29.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.3.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.3.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:              blk.3.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:            blk.3.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:           blk.3.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:         blk.3.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:                blk.3.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:              blk.3.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:              blk.3.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:            blk.3.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:            blk.30.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.30.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.4.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:           blk.4.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:              blk.4.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:            blk.4.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.4.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:         blk.4.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:                blk.4.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:              blk.4.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:              blk.4.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:            blk.4.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.5.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.5.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:              blk.5.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:            blk.5.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.5.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:         blk.5.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:                blk.5.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:              blk.5.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:              blk.5.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:            blk.5.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.6.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.6.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:              blk.6.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:            blk.6.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.6.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:         blk.6.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:                blk.6.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:              blk.6.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:              blk.6.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:            blk.6.ffn_down.weight q5_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.7.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.7.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:              blk.7.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:            blk.7.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.7.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:         blk.7.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:                blk.7.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:              blk.7.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:              blk.7.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:            blk.7.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.8.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.8.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:              blk.8.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:            blk.8.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.8.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:         blk.8.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:                blk.8.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:              blk.8.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:              blk.8.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:            blk.8.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.9.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:           blk.9.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:              blk.9.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:            blk.9.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.9.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:         blk.9.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:                blk.9.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:              blk.9.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:              blk.9.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:            blk.9.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:                      output.bias f32      [ 51200,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:                    output.weight q6_K     [  2560, 51200,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:                 output_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:               output_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.30.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.30.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:          blk.30.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:        blk.30.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:               blk.30.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:             blk.30.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:             blk.30.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.30.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:            blk.31.attn_norm.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.31.attn_norm.weight f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:             blk.31.attn_qkv.bias f32      [  7680,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.31.attn_qkv.weight q6_K     [  2560,  7680,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:          blk.31.attn_output.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:        blk.31.attn_output.weight q5_K     [  2560,  2560,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:               blk.31.ffn_up.bias f32      [ 10240,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.31.ffn_up.weight q5_K     [  2560, 10240,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:             blk.31.ffn_down.bias f32      [  2560,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.31.ffn_down.weight q6_K     [ 10240,  2560,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type q5_K:   81 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 10240\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 2.78 B\n",
      "llm_load_print_meta: model size       = 1.93 GiB (5.96 BPW) \n",
      "llm_load_print_meta: general.name     = Phi2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   86.06 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors: VRAM used: 1889.00 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 774/774\n",
      "llama_new_context_with_model: compute buffer total size = 165.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 162.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 2051.01 MiB (model: 1889.00 MiB, context: 162.00 MiB)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"/home/ruzickal/.cache/huggingface/hub/models--TheBloke--phi-2-GGUF/snapshots/5a454d977c6438bb9fb2df233c8ca70f21c87420/phi-2.Q5_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"Instruct: {prompt}\\nOutput:\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant'}\n",
      "{'content': '\\n'}\n",
      "{'content': 'The'}\n",
      "{'content': ' Story'}\n",
      "{'content': ' of'}\n",
      "{'content': ' the'}\n",
      "{'content': ' Ll'}\n",
      "{'content': 'ama'}\n",
      "{'content': '\\n'}\n",
      "{'content': 'L'}\n",
      "{'content': 'lam'}\n",
      "{'content': 'as'}\n",
      "{'content': ' are'}\n",
      "{'content': ' one'}\n",
      "{'content': ' of'}\n",
      "{'content': ' the'}\n",
      "{'content': ' most'}\n",
      "{'content': ' interesting'}\n",
      "{'content': ' animals'}\n",
      "{'content': ' in'}\n",
      "{'content': ' the'}\n",
      "{'content': ' world'}\n",
      "{'content': '.'}\n",
      "{'content': ' They'}\n",
      "{'content': ' have'}\n",
      "{'content': ' been'}\n",
      "{'content': ' domest'}\n",
      "{'content': 'icated'}\n",
      "{'content': ' for'}\n",
      "{'content': ' thousands'}\n",
      "{'content': ' of'}\n",
      "{'content': ' years'}\n",
      "{'content': ' and'}\n",
      "{'content': ' are'}\n",
      "{'content': ' used'}\n",
      "{'content': ' for'}\n",
      "{'content': ' transportation'}\n",
      "{'content': ','}\n",
      "{'content': ' wool'}\n",
      "{'content': ' production'}\n",
      "{'content': ','}\n",
      "{'content': ' and'}\n",
      "{'content': ' even'}\n",
      "{'content': ' as'}\n",
      "{'content': ' therapy'}\n",
      "{'content': ' animals'}\n",
      "{'content': '.'}\n",
      "{'content': ' In'}\n",
      "{'content': ' this'}\n",
      "{'content': ' article'}\n",
      "{'content': ','}\n",
      "{'content': ' we'}\n",
      "{'content': ' will'}\n",
      "{'content': ' explore'}\n",
      "{'content': ' the'}\n",
      "{'content': ' fascinating'}\n",
      "{'content': ' history'}\n",
      "{'content': ' and'}\n",
      "{'content': ' characteristics'}\n",
      "{'content': ' of'}\n",
      "{'content': ' ll'}\n",
      "{'content': 'amas'}\n",
      "{'content': '.'}\n",
      "{'content': '\\n'}\n",
      "{'content': 'L'}\n",
      "{'content': 'l'}\n",
      "{'content': 'ama'}\n",
      "{'content': ' History'}\n",
      "{'content': '\\n'}\n",
      "{'content': 'The'}\n",
      "{'content': ' origin'}\n",
      "{'content': ' of'}\n",
      "{'content': ' ll'}\n",
      "{'content': 'amas'}\n",
      "{'content': ' is'}\n",
      "{'content': ' still'}\n",
      "{'content': ' a'}\n",
      "{'content': ' mystery'}\n",
      "{'content': ' to'}\n",
      "{'content': ' scientists'}\n",
      "{'content': ','}\n",
      "{'content': ' but'}\n",
      "{'content': ' it'}\n",
      "{'content': ' is'}\n",
      "{'content': ' believed'}\n",
      "{'content': ' that'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m stream \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a story writing assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m     14\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py:222\u001b[0m, in \u001b[0;36m_convert_text_completion_chunks_to_chat\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_text_completion_chunks_to_chat\u001b[39m(\n\u001b[1;32m    220\u001b[0m     chunks: Iterator[llama_types\u001b[38;5;241m.\u001b[39mCreateCompletionStreamResponse],\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[llama_types\u001b[38;5;241m.\u001b[39mChatCompletionChunk]:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[1;32m    225\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 ],\n\u001b[1;32m    238\u001b[0m             }\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama.py:1473\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1471\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1472\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1474\u001b[0m     prompt_tokens,\n\u001b[1;32m   1475\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1476\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1477\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1478\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1479\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1480\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1481\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1482\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1483\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1484\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1485\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1486\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1487\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1488\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1489\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1490\u001b[0m ):\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m   1492\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1264\u001b[0m     )\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   1267\u001b[0m     ):\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m   1068\u001b[0m )\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/LLM/venv/lib/python3.10/site-packages/llama_cpp/llama_cpp.py:1475\u001b[0m, in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stream = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about llamas.\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "result = []\n",
    "for message in stream:\n",
    "    result.append(message)\n",
    "    print(message[\"choices\"][0][\"delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.n_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 51200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(llm.eval_logits).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' change'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = llm.tokenize(\"AI is going to \")\n",
    "tokens = llm.prepare_inputs_for_generation(tokens)\n",
    "llm.eval(tokens)\n",
    "logits = np.array(llm.logits)\n",
    "selection = np.argmax(logits, axis=-1)\n",
    "llm.detokenize([selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(llm.logits).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(top_k=40, top_p=0.95, temperature=0.8, repetition_penalty=1.1, last_n_tokens=64, seed=-1, batch_size=8, threads=-1, max_new_tokens=256, stop=None, stream=False, reset=True, context_length=-1, gpu_layers=12, mmap=True, mlock=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
